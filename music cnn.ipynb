{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "#畳み込み層の実装\n",
    "def conv_layer(input_tensor,name,n_output_channels,ksize,padding='SAME',strides=[1,1,1,1]):\n",
    "    with tf.variable_scope(name):\n",
    "        n_input_channels=1\n",
    "        \n",
    "        weights_shape=(list(ksize)+[n_input_channels,n_output_channels])\n",
    "        weights=tf.get_variable(name='_weights',shape=weights_shape)\n",
    "        \n",
    "        print(weights)\n",
    "        \n",
    "        biases=tf.get_variable(name='_biases',initializer=tf.zeros(shape=[n_output_channels]))\n",
    "        \n",
    "        print(biases)\n",
    "        #重みと入力テンソルの掛け算\n",
    "        conv=tf.nn.conv2d(input=input_tensor,filter=weights,strides=strides,padding=padding)\n",
    "        print(conv)\n",
    "        #重みと入力テンソルの積にバイアスを加える\n",
    "        conv=tf.nn.bias_add(conv,biases,name='net_pre_activation')\n",
    "        print(conv)\n",
    "        #線形活性化\n",
    "        conv=tf.nn.relu(conv,name='activation')\n",
    "        print(conv)\n",
    "        \n",
    "        return conv        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#全結合層の実装\n",
    "def fc_layer(input_tensor,name,n_output_units,activation_fn=None):\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape=input_tensor.get_shape().as_list()[1:]\n",
    "        n_input_units=np.prod(input_shape)\n",
    "        \n",
    "        if len(input_shape)>1:\n",
    "            input_tensor=tf.reshape(input_tensor,shape=(-1,n_input_units))\n",
    "            \n",
    "        weights_shape=[n_input_units,n_output_units]\n",
    "        weights=tf.get_variable(name='__weights',shape=weights_shape)\n",
    "        print(weights)\n",
    "        \n",
    "        biases=tf.get_variable(name='__biases',initializer=tf.zeros(shape=[n_output_units]))\n",
    "        print(biases)\n",
    "        #入力テンソルと重みの積の総和\n",
    "        layer=tf.matmul(input_tensor,weights)\n",
    "        print(layer)\n",
    "        #バイアスが入る\n",
    "        layer=tf.nn.bias_add(layer,biases,name='net_pre_activation')\n",
    "        print(layer)\n",
    "        \n",
    "        if activation_fn is None:\n",
    "            return layer\n",
    "        layer=activation_fn(layer,name='activation')\n",
    "        print(layer)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model構築\n",
    "def build():\n",
    "    #784のところは学習させる画像のピクセル数による\n",
    "    #画像を学習させるときは一次元配列に直す必要性あり\n",
    "    tf_x=tf.placeholder(tf.float32,shape=[None,784],name='tf_x')\n",
    "    tf_y=tf.placeholder(tf.int32,shape=[None],name='tf_y')\n",
    "    \n",
    "    #入力テンソルはバッチ学習ができるように四次元テンソルにしておく\n",
    "    tf_x_img=tf.reshape(tf_x,shape=[-1,int(np.sqrt()),int(np.sqrt()),1],name='tf_x_reshape')\n",
    "    tf_y_onehot=tf.one_hot(indices=tf_y,depth=8,dtype=tf.float32,name='tf_y_onehot')\n",
    "    \n",
    "    #第一層\n",
    "    print('==building 1st layer==')\n",
    "    h1=conv_layer(tf_x_img,name='conv_1',n_output_channels=16,ksize=(6,6),padding='VALID')\n",
    "    \n",
    "    print('==h1 pooling==')\n",
    "    h1_pool=tf.nn.max_pool(h1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    \n",
    "    #第二層\n",
    "    print('==building 2nd layer==')\n",
    "    h2=conv_layer(h1_pool,name='conv_2',n_output_channels=32,ksize=(6,6),padding='VALID')\n",
    "    \n",
    "    print('==h2 pooling==')\n",
    "    h2_pool=tf.nn.max_pool(h2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    \n",
    "    #第三層\n",
    "    print('==building 3rd layer==')\n",
    "    h3=conv_layer(h2_pool,name='conv_3',n_output_channels=64,ksize=(4,4),padding='VALID')\n",
    "    \n",
    "    print('==h3 pooling==')\n",
    "    h3_pool=tf.nn.max_pool(h3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    \n",
    "    #第四層\n",
    "    h4=fc_layer(h3_pool,name='linear_4',n_output_units=4096,activation_fn=tf.nn.relu)\n",
    "    \n",
    "    #dropout\n",
    "    keep_prob=tf.placeholder(tf.float32,name='tf_keep_prob')\n",
    "    h4_drop=tf.nn.dropout(h3_pool,keep_prob=keep_prob,name='dropout_layer')\n",
    "    \n",
    "    #第五層\n",
    "    #classの数がn_output_units \n",
    "    #class数はアーテイストの数による\n",
    "    h5=fc_layer(h4_drop,name='linear_5',n_output_units=7,activation_fn=None)\n",
    "    \n",
    "    \n",
    "    #予測とラベル\n",
    "    predictions={'probabilities':tf.nn.softmax(h5,name='probabilities'),'labels':\n",
    "                tf.cast(tf.argmax(h5,axis=1),tf.int32,name='labels')}\n",
    "    \n",
    "    #損失関数、コスト\n",
    "    cost=tf.reduce_mean(tf.softmax_cross_entropy_with_logits(logits=h5,labels=tf_y_onehot),name=(cross_entropy_loss))\n",
    "    \n",
    "    #最適化\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer=optimizer.minimize(cost,name='train_op')\n",
    "    \n",
    "    #予測正解率を特定\n",
    "    correct_predictions=tf.eqaul(predictions['labels'],tf_y,name='correct_pred')\n",
    "    \n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_predictions,tf.float32),name='accuracy')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train:学習をするアルゴリズムを構築する関数\\n   save:学習したモデルを保存する関数\\n   load:学習したモデルを呼び出す関数\\n   predict:テストデータと比較する関数'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train:学習をするアルゴリズムを構築する関数\n",
    "   save:学習したモデルを保存する関数\n",
    "   load:学習したモデルを呼び出す関数\n",
    "   predict:テストデータと比較する関数\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#バッチ学習\n",
    "def batch_generator(X_train,y_train,shuffle=True,batch_size=64,random_seed=None):\n",
    "    idx=np.arange(y_train.shape[0])\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.RandomState(random_state).shuffle(idx)\n",
    "        X=X_train[idx]\n",
    "        y=y_train[idx]\n",
    "        \n",
    "        \n",
    "    for i in range(0,X.shape[0],batch_size):\n",
    "        yield(X[i*i+batch_size,:],y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train\n",
    "def train(sess,training_set,validation_set=None,initialize=True,epochs=20,\n",
    "          shuffle=True,dropout=0.5,random_seed=None):\n",
    "    #training dataの成型\n",
    "    X_train=np.array(training_set[0])\n",
    "    y_train=np.array(training_set[1])\n",
    "    \n",
    "    #training_lossを評価\n",
    "    training_loss=[]\n",
    "    \n",
    "    if initialize:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    #バッチ学習\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    \n",
    "    #学習スタート\n",
    "    for epoch in range(epochs):\n",
    "        bach_gen=batch_generator(X_train,y_train,shuffle=shuffle)\n",
    "        avg_loss=0.0\n",
    "        \n",
    "        for idx,(batch_x,batch_y) in enumerate(batch_gen):\n",
    "            feed={'tf_x:0':batch_x,'tf_y:0':batch_y,'tf_keep_prob:0':dropout}\n",
    "            loss,_=sess.run(['cost:0','train_op'],feed_dict=feed)\n",
    "            avg_loss+=loss\n",
    "            \n",
    "        training_loss.append(avg_loss/(idx+1))\n",
    "        print('epoch%d training_loss:%.7f'%(epoch,avg_loss))\n",
    "        \n",
    "        #検証データ、検証するとき\n",
    "        if validation_set is not None:\n",
    "            feed={'tf_x:0':validation_set[0],'tf_y:0':validation_set[1],'tf_keep_prob:0':1.0}\n",
    "            \n",
    "            valid_acc=sess.run('accuracy:0',feed_dict=feed)\n",
    "            \n",
    "            print('validation acc%.5f'%(valid_acc))\n",
    "            \n",
    "        else:\n",
    "            print()\n",
    "            \n",
    "#save\n",
    "def save(saver,sess,epoch,path='./model/'):\n",
    "    #saverのインスタンスはsaver=tf.train.saver()\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    print('saving_model in &s'%path)\n",
    "    saver.save(sess,os.path.join(path,'cnn-model.ckpt'),global_step=epoch)\n",
    "    \n",
    "def load(saver,sess,path,epoch):\n",
    "    print('loading model from %s'%path)\n",
    "    saver.restore(sess,os.path.join(path,'cnn-model.ckpt-%d'%epoch))\n",
    "    \n",
    "#predict\n",
    "#return_probaは確率的スコアリングをするかしないかを決める\n",
    "def predict(sess,X_test,return_proba=False):\n",
    "    feed={'tf_x:0':X_test,'tf_keep_prob:0':1.0}\n",
    "    if return_proba:\n",
    "        return sess.run('probabilities:0',feed_dict=feed)\n",
    "    else:\n",
    "        return sess.run('labels:0',feed_dict=feed)\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#音楽の曲調をケプストラム分析で抽出し、画像データに変換してデータセットにする\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.io.wavefile import read\n",
    "\n",
    "def __init__(self,wavfile,chunk,length,expected_fs=None):\n",
    "    fs,all_data=read(wavfile)\n",
    "    if expected_fs!=None and expected_fs!=fs:\n",
    "        print('It has difference between expected_fs and fs')\n",
    "        raise AssertionError\n",
    "    \n",
    "    #dataの形を成型\n",
    "    all_data=all_data.astype('float64')-128.0\n",
    "    all_data/=128.0\n",
    "    \n",
    "    self.data=all_data\n",
    "    self.sampling_rate=fs\n",
    "    self.chunk=chunk\n",
    "    self.length=length\n",
    "    \n",
    "    #noise読み込み\n",
    "    self.noise=np.load('noise/noise.npy')\n",
    "    self.noise=self.noise.astype('float32')/128.0\n",
    "    \n",
    "    \n",
    "    #index??\n",
    "    n_blocks_all=len(all_data)-self.chunk*self.length-1\n",
    "    self.indexes=np.linspace(0,n_blocks_all,int(n_blocks_all/5.0))\n",
    "    self.n_blocks=len(self.indexes)\n",
    "    \n",
    "    print('sampling_rate:{}'.format(fs))\n",
    "    \n",
    "    \n",
    "def shuffle_indexes(self):\n",
    "    self.indexes=np.random.permutation(len(self.indexes))\n",
    "    \n",
    "#ノイズを追加\n",
    "def add_noise(self,data,scale=None):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
